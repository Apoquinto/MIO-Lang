{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analex\n",
    "En este caso, como no podemos leer el archivo en un jupyter, este solo sirve para comprobar que funcione bien la identificacion de tokens por parte de nuestras expresiones regulares ademas de algunos algoritmos interesantes como el que se creo para tokenizar las lineas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import *\n",
    "import re\n",
    "\n",
    "class Analex:\n",
    "    def __init__(self):    \n",
    "        self.dir = ''\n",
    "        self.source = ''\n",
    "        self.sourceCode = []\n",
    "        self.tokens = []\n",
    "        self.IDcount = 0\n",
    "        self.TXTcount = 0\n",
    "        self.reserveds = ['PROGRAMA', 'FINPROG', 'SI', 'ENTONCES', 'SINO', 'FINSI', 'REPITE', 'VECES', 'FINREP', 'IMPRIME', 'LEE']\n",
    "        self.relationals = ['>', '<', '==', '=']\n",
    "        self.aritmetics = ['+', '-', '*', '/']\n",
    "        self.numerics = []\n",
    "        self.ids = []\n",
    "        self.texts = []\n",
    "\n",
    "    # Recibe un string de elementos\n",
    "    # Devuelve los tokens del string\n",
    "    def tokenizar(self, line):\n",
    "        # Bandera para reconocer si hay un '\"'\n",
    "        comillaActiva = False\n",
    "        # Se le agrega un espacio para que automaticamente el algortimo guarde el ultimo token\n",
    "        line += ' '\n",
    "        # Este es el constructor de tokens, ira guardando los caracteres hasta que se considere \n",
    "        # que ya no es parte de un token\n",
    "        tokenAux = ''\n",
    "        # Aqui guardamos los tokens finales de la lista\n",
    "        tokens = []\n",
    "        # Obtenemos el tama;o de la linea\n",
    "        tam = len(line)\n",
    "        # recorremos caracter por caracter\n",
    "        for i in range(tam):\n",
    "            # En caso de que se reconozca que es un texto, se segira otra regla para encontrar el token.\n",
    "            if comillaActiva:\n",
    "                # Primero buscamos priorizar su final encontrando su pareja.\n",
    "                if(line[i] == '\"'):\n",
    "                    tokenAux += line[i]\n",
    "                    comillaActiva = False\n",
    "                # Agregamos una excepcion por si al final no existe la pareja\n",
    "                elif(tam == i + 1):\n",
    "                    tokens.append(tokenAux)                \n",
    "                # Por definicion, se debe guardar como token incluso los espacios dentro de las comillas\n",
    "                # Por lo que literalmente guardamos todo en nuestro token temporal hasta que encontremos\n",
    "                # La pareja, es decir, se cambie \n",
    "                else:\n",
    "                    tokenAux += line[i]\n",
    "            else:\n",
    "                # Si no es ni espacio en blanco ni comilla, entonces se agrega al token actual.\n",
    "                if line[i] != ' ' and line[i] != '\"':\n",
    "                    tokenAux += line[i]\n",
    "                # En caso de iniciar una comilla[texto] activa una bandera.\n",
    "                elif(line[i] == '\"'):\n",
    "                    tokenAux += line[i]\n",
    "                    comillaActiva = True\n",
    "                # Como ya sabemos que tiene que ser un espacio en blanco, agregamos el token actual \n",
    "                # a los tokens de la linea y reiniciamos el token actual\n",
    "                else:\n",
    "                    tokens.append(tokenAux)\n",
    "                    tokenAux = ''\n",
    "        return tokens\n",
    "        \n",
    "    def isSyntax(self, token):\n",
    "        return (token in self.reserveds or \n",
    "                token in self.aritmetics or \n",
    "                token in self.relationals)\n",
    "\n",
    "    def isNumeric(self, token):\n",
    "        flag = False\n",
    "        if(re.fullmatch('[0-8]+', token)):\n",
    "            flag = True\n",
    "        return flag\n",
    "    \n",
    "    def isID(self, token):\n",
    "        aux = token\n",
    "        flag = re.fullmatch('[a-zA-Z][a-zA-Z0-9]+', aux)\n",
    "        return flag\n",
    "    \n",
    "    def isText(self, token):\n",
    "        flag = re.fullmatch('\"[a-zA-Z0-9\\s]+\"', token)\n",
    "        return flag\n",
    "\n",
    "    # retorna el valor decimal de un valor octal\n",
    "    def octal_a_decimal(self, octal):\n",
    "        decimal = 0\n",
    "        posicion = 0\n",
    "        # Invertir octal, porque debemos recorrerlo de derecha a izquierda\n",
    "        # pero for in empieza de izquierda a derecha\n",
    "        octal = octal[::-1]\n",
    "        for digito in octal:\n",
    "            valor_entero = int(digito)\n",
    "            numero_elevado = int(8 ** posicion)\n",
    "            equivalencia = int(numero_elevado * valor_entero)\n",
    "            decimal += equivalencia\n",
    "            posicion += 1\n",
    "        return decimal\n",
    "    \n",
    "analex = Analex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['\"Buenos dias\"','PROGRAM', 'FINPROGRAM', '\"HOLA\"','ENTONCES','SI', '321', '812', 'SINO', 'PROGRAMA', 'resultado','\"BUENAS\"','d23','1239']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTONCES\n",
      "SI\n",
      "SINO\n",
      "PROGRAMA\n"
     ]
    }
   ],
   "source": [
    "# Test de Syntax\n",
    "\n",
    "for token in test:\n",
    "    if(analex.isSyntax(token)):\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "812\n"
     ]
    }
   ],
   "source": [
    "# Test de numers\n",
    "\n",
    "for token in test:\n",
    "    if(analex.isNumeric(token)):\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Buenos dias\"\n",
      "\"HOLA\"\n",
      "\"BUENAS\"\n",
      "\"Factorial de \"\n",
      "\" es \"\n"
     ]
    }
   ],
   "source": [
    "# Test de textos\n",
    "\n",
    "for token in test:\n",
    "    if(analex.isText(token)):\n",
    "        print(token)\n",
    "        \n",
    "# Test Extra\n",
    "t = ['IMPRIME', '\"Factorial de \"', 'IMPRIME', 'Num', 'IMPRIME', '\" es \"']\n",
    "for token in t:\n",
    "    if(analex.isText(token)):\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGRAM\n",
      "FINPROGRAM\n",
      "ENTONCES\n",
      "SI\n",
      "SINO\n",
      "PROGRAMA\n",
      "resultado\n",
      "d23\n"
     ]
    }
   ],
   "source": [
    "# Test de IDs\n",
    "\n",
    "for token in test:\n",
    "    if(analex.isID(token)):\n",
    "        print(token)\n",
    "        \n",
    "# En este caso como se esta haciendo la evaluacion por separado, agarra tambien las syntax, pero\n",
    "# en la implementacion primero tiene que pasar la prueba de que son syntax y si no son sintax se \n",
    "# evalua como ID, ahorrandonos operaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola,', '\"', 'me', 'llamo', 'apoquinto\"']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizador simple, no considera los textos con espacoos\n",
    "def tokenizar(line):\n",
    "    line += ' '\n",
    "    tokenAux = ''\n",
    "    tokens = []\n",
    "    tam = len(line)\n",
    "    for i in range(tam):\n",
    "        if line[i] != ' ':\n",
    "            tokenAux += line[i]\n",
    "        else:\n",
    "            tokens.append(tokenAux)\n",
    "            tokenAux = ''\n",
    "    return tokens\n",
    "print(tokenizar('Hola, \" me llamo apoquinto\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola,', '\" me llamo apoquinto\"']\n",
      "['Hola,', 'me', 'llamo', 'apoquinto']\n",
      "['Hola,', 'me', 'llamo', '\"apoquinto']\n",
      "['Hola,', 'me', 'llamo', '\"apoquinto\"']\n",
      "['Hola,', 'me', 'llamo', '\"\"Apoquinto\"\"']\n",
      "['Hola,', 'me', 'llamo', '\"\"A\"\"']\n",
      "['IMPRIME', '\"Factorial de \"', 'IMPRIME', 'Num', 'IMPRIME', '\" es \"']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizador completo\n",
    "# Recibe un string de elementos\n",
    "# Devuelve los tokens del string\n",
    "def tokenizar(line):\n",
    "    # Bandera para reconocer si hay un '\"'\n",
    "    comillaActiva = False\n",
    "    # Se le agrega un espacio para que automaticamente el algortimo guarde el ultimo token\n",
    "    line += ' '\n",
    "    # Este es el constructor de tokens, ira guardando los caracteres hasta que se considere \n",
    "    # que ya no es parte de un token\n",
    "    tokenAux = ''\n",
    "    # Aqui guardamos los tokens finales de la lista\n",
    "    tokens = []\n",
    "    # Obtenemos el tama;o de la linea\n",
    "    tam = len(line)\n",
    "    # recorremos caracter por caracter\n",
    "    for i in range(tam):\n",
    "        # En caso de que se reconozca que es un texto, se segira otra regla para encontrar el token.\n",
    "        if comillaActiva:\n",
    "            # Primero buscamos priorizar su final encontrando su pareja.\n",
    "            if(line[i] == '\"'):\n",
    "                tokenAux += line[i]\n",
    "                comillaActiva = False\n",
    "            # Agregamos una excepcion por si al final no existe la pareja\n",
    "            elif(tam == i + 1):\n",
    "                tokens.append(tokenAux)                \n",
    "            # Por definicion, se debe guardar como token incluso los espacios dentro de las comillas\n",
    "            # Por lo que literalmente guardamos todo en nuestro token temporal hasta que encontremos\n",
    "            # La pareja, es decir, se cambie \n",
    "            else:\n",
    "                tokenAux += line[i]\n",
    "        else:\n",
    "            # Si no es ni espacio en blanco ni comilla, entonces se agrega al token actual.\n",
    "            if line[i] != ' ' and line[i] != '\"':\n",
    "                tokenAux += line[i]\n",
    "            # En caso de iniciar una comilla[texto] activa una bandera.\n",
    "            elif(line[i] == '\"'):\n",
    "                tokenAux += line[i]\n",
    "                comillaActiva = True\n",
    "            # Como ya sabemos que tiene que ser un espacio en blanco, agregamos el token actual \n",
    "            # a los tokens de la linea y reiniciamos el token actual\n",
    "            else:\n",
    "                tokens.append(tokenAux)\n",
    "                tokenAux = ''\n",
    "    return tokens\n",
    "\n",
    "#Pruebas de funcionamiento:\n",
    "# Caso rudo\n",
    "print(tokenizar('Hola, \" me llamo apoquinto\"'))\n",
    "# Caso normal\n",
    "print(tokenizar('Hola, me llamo apoquinto'))\n",
    "# Caso en el que la comilla no termina\n",
    "print(tokenizar('Hola, me llamo \"apoquinto'))\n",
    "# Caso en la que la comilla si termina\n",
    "print(tokenizar('Hola, me llamo \"apoquinto\"'))\n",
    "# Caso en la que hay doble comlla\n",
    "print(tokenizar('Hola, me llamo \"\"Apoquinto\"\"'))\n",
    "# Caso para comilla doble pero con letra intermedia\n",
    "print(tokenizar('Hola, me llamo \"\"A\"\"'))\n",
    "# Prueba final\n",
    "print(tokenizar('IMPRIME \"Factorial de \" IMPRIME Num IMPRIME \" es \"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba del tokenizador y el detector de "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
